{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 Analysis: Bias Detection in ChatGPT\n",
    "\n",
    "This notebook analyzes the results from Phase 1 and creates visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Phase 1 results\n",
    "with open('../results/phase1/phase1_results.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print('Results loaded successfully!')\n",
    "print(f\"Model: {results['metadata']['model']}\")\n",
    "print(f\"Number of profiles: {results['metadata']['num_profiles']}\")\n",
    "print(f\"Timestamp: {results['metadata']['timestamp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fairness metrics\n",
    "metrics = results['metrics']\n",
    "\n",
    "# Demographic Parity\n",
    "dp_gender = metrics['demographic_parity']['gender']['score']\n",
    "dp_age = metrics['demographic_parity']['age']['score']\n",
    "\n",
    "# Individual Fairness\n",
    "if_score = metrics['individual_fairness']['score']\n",
    "\n",
    "# Equal Opportunity\n",
    "eo_score = metrics['equal_opportunity']['score']\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Demographic Parity (Gender)',\n",
    "        'Demographic Parity (Age)',\n",
    "        'Individual Fairness',\n",
    "        'Equal Opportunity'\n",
    "    ],\n",
    "    'Score': [dp_gender, dp_age, if_score, eo_score],\n",
    "    'Target': [0.8, 0.8, 0.8, 0.8]\n",
    "})\n",
    "\n",
    "summary_df['Status'] = summary_df['Score'].apply(\n",
    "    lambda x: '✓ Fair' if x >= 0.8 else '⚠ Moderate Bias' if x >= 0.7 else '✗ Significant Bias'\n",
    ")\n",
    "\n",
    "print('\\nFairness Metrics Summary:')\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Overall Fairness Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create bar plot\n",
    "bars = ax.bar(summary_df['Metric'], summary_df['Score'], color='steelblue', alpha=0.7)\n",
    "\n",
    "# Add target line\n",
    "ax.axhline(y=0.8, color='green', linestyle='--', label='Fair Threshold (0.8)', linewidth=2)\n",
    "ax.axhline(y=0.7, color='orange', linestyle='--', label='Moderate Threshold (0.7)', linewidth=2)\n",
    "\n",
    "# Formatting\n",
    "ax.set_ylabel('Fairness Score', fontsize=12)\n",
    "ax.set_title('Phase 1: Fairness Metrics Overview', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Rotate x labels\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.3f}',\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/visualizations/phase1_fairness_overview.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Visualization saved to results/visualizations/phase1_fairness_overview.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Demographic Parity by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract gender distribution data\n",
    "gender_dist = metrics['demographic_parity']['gender']['details']['distributions']\n",
    "\n",
    "# Create comparison plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Divergences between groups\n",
    "divergences = metrics['demographic_parity']['gender']['details']['divergences']\n",
    "axes[0].hist(divergences, bins=20, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('JS Divergence', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0].set_title('Distribution Divergences Between Gender Groups', fontsize=12, fontweight='bold')\n",
    "axes[0].axvline(np.mean(divergences), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(divergences):.3f}', linewidth=2)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Demographic parity score\n",
    "genders = list(gender_dist.keys())\n",
    "scores = [1.0 - d for d in divergences[:len(genders)]]  # Convert to fairness scores\n",
    "\n",
    "if len(genders) >= 2:\n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71'][:len(genders)]\n",
    "    bars = axes[1].bar(genders, [1]*len(genders), color=colors, alpha=0.5, label='Ideal (Perfect Fairness)')\n",
    "    bars2 = axes[1].bar(genders, scores, color=colors, alpha=0.9, label='Actual')\n",
    "    \n",
    "    axes[1].set_ylabel('Fairness Score', fontsize=11)\n",
    "    axes[1].set_title('Demographic Parity by Gender', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylim(0, 1.1)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/visualizations/phase1_gender_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Visualization saved to results/visualizations/phase1_gender_analysis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Individual Fairness Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract individual fairness similarity scores\n",
    "similarity_scores = metrics['individual_fairness']['details']['similarity_scores']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create histogram\n",
    "ax.hist(similarity_scores, bins=30, color='mediumseagreen', alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add mean line\n",
    "mean_sim = np.mean(similarity_scores)\n",
    "ax.axvline(mean_sim, color='red', linestyle='--', \n",
    "           label=f'Mean Similarity: {mean_sim:.3f}', linewidth=2)\n",
    "\n",
    "# Add fairness threshold\n",
    "ax.axvline(0.8, color='green', linestyle='--', \n",
    "           label='Fair Threshold (0.8)', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Jaccard Similarity Score', fontsize=11)\n",
    "ax.set_ylabel('Number of Profile Pairs', fontsize=11)\n",
    "ax.set_title('Individual Fairness: Similarity Between Identical Profiles\\n(Higher = More Fair)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/visualizations/phase1_individual_fairness.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Visualization saved to results/visualizations/phase1_individual_fairness.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interpretation and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('PHASE 1 INTERPRETATION')\n",
    "print('='*80)\n",
    "print()\n",
    "\n",
    "# Overall assessment\n",
    "avg_fairness = np.mean([dp_gender, dp_age, if_score, eo_score])\n",
    "\n",
    "print(f'Overall Fairness Score: {avg_fairness:.4f}')\n",
    "print()\n",
    "\n",
    "if avg_fairness >= 0.8:\n",
    "    print('✓ RESULT: ChatGPT shows generally FAIR behavior in recommendations')\n",
    "elif avg_fairness >= 0.7:\n",
    "    print('⚠ RESULT: ChatGPT shows MODERATE BIAS in recommendations')\n",
    "else:\n",
    "    print('✗ RESULT: ChatGPT shows SIGNIFICANT BIAS in recommendations')\n",
    "\n",
    "print()\n",
    "print('Key Findings:')\n",
    "print('-' * 80)\n",
    "\n",
    "if dp_gender < 0.8:\n",
    "    print(f'• Gender bias detected (score: {dp_gender:.3f})')\n",
    "    print('  Different recommendations for users with identical preferences but different genders')\n",
    "\n",
    "if if_score < 0.8:\n",
    "    print(f'• Individual fairness issues (score: {if_score:.3f})')\n",
    "    print('  Similar users receive dissimilar recommendations')\n",
    "\n",
    "if eo_score < 0.8:\n",
    "    print(f'• Equal opportunity concerns (score: {eo_score:.3f})')\n",
    "    print('  High-quality items not equally recommended across groups')\n",
    "\n",
    "print()\n",
    "print('Implications:')\n",
    "print('-' * 80)\n",
    "print('• LLM-based recommendation systems may perpetuate demographic biases')\n",
    "print('• Users from different demographic groups may receive systematically different recommendations')\n",
    "print('• Bias mitigation techniques (Phase 3) are necessary for fair deployments')\n",
    "print()\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "report = f\"\"\"\n",
    "PHASE 1: BIAS DETECTION IN CHATGPT - SUMMARY REPORT\n",
    "{'='*80}\n",
    "\n",
    "Model: {results['metadata']['model']}\n",
    "Date: {results['metadata']['timestamp']}\n",
    "Profiles Tested: {results['metadata']['num_profiles']}\n",
    "\n",
    "FAIRNESS METRICS (0-1 scale, higher is better):\n",
    "{'-'*80}\n",
    "Demographic Parity (Gender):  {dp_gender:.4f}\n",
    "Demographic Parity (Age):     {dp_age:.4f}\n",
    "Individual Fairness:          {if_score:.4f}\n",
    "Equal Opportunity:            {eo_score:.4f}\n",
    "\n",
    "Average Fairness:             {avg_fairness:.4f}\n",
    "\n",
    "STATUS: {'FAIR' if avg_fairness >= 0.8 else 'MODERATE BIAS' if avg_fairness >= 0.7 else 'SIGNIFICANT BIAS'}\n",
    "\n",
    "INTERPRETATION:\n",
    "{'-'*80}\n",
    "{'Gender bias detected - different recommendations for different genders' if dp_gender < 0.8 else 'No significant gender bias detected'}\n",
    "{'Individual fairness issues - similar users get different recommendations' if if_score < 0.8 else 'Good individual fairness'}\n",
    "{'Equal opportunity concerns - biased item recommendations' if eo_score < 0.8 else 'Good equal opportunity'}\n",
    "\n",
    "NEXT STEPS:\n",
    "{'-'*80}\n",
    "1. Run Phase 2 to compare with other models\n",
    "2. Run Phase 3 to test bias mitigation\n",
    "3. Analyze visualizations in results/visualizations/\n",
    "\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "# Save report\n",
    "with open('../results/phase1/phase1_summary_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(report)\n",
    "print('\\nReport saved to results/phase1/phase1_summary_report.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
